{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_regnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g488Jy0kf2xZ",
        "outputId": "9f039b7d-a699-4535-a894-40b0d701d2e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "d0xLeYVNtNva"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data array and create dataframe"
      ],
      "metadata": {
        "id": "gQQF2CQMvtcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'train.zip'\n",
        "!unzip 'label.zip'"
      ],
      "metadata": {
        "id": "MCzmJtmMriDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OhCsnxPcvss8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e10aa6-d4cd-423c-f730-cb7cf80e46b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3777,)\n",
            "(3777,)\n"
          ]
        }
      ],
      "source": [
        "npzfile = np.load('train_data.npz')\n",
        "x_arr = npzfile['x']\n",
        "y_arr = npzfile['y']\n",
        "\n",
        "print(x_arr.shape)\n",
        "print(y_arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\n",
        "    \"filename\": x_arr, \n",
        "    \"label\": y_arr,\n",
        "}\n",
        "df = pd.DataFrame(dic)"
      ],
      "metadata": {
        "id": "yYzB9d30sgLC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "gD4OsqQ1soIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "-dKRZEidwxBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyper = {\n",
        "    \"batch_size\": 32,\n",
        "    \"img_size\": 285,\n",
        "    \"class_num\": 8,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"step_size\": [36, 40, 44],\n",
        "    \"epoch\": 50,\n",
        "}"
      ],
      "metadata": {
        "id": "LxwTYejlwvua"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataset"
      ],
      "metadata": {
        "id": "62XotXAtr6Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fish_dataset(Dataset):\n",
        "    def __init__(self, dataframe, training=False, rootPath='', img_size=hyper[\"img_size\"]):\n",
        "        self.rootPath = rootPath\n",
        "        self.dataframe = dataframe\n",
        "        self.img_size = img_size\n",
        "        self.training = training\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.training and random.random() < 0.8:\n",
        "          img, label = self.copy_paste(index)\n",
        "        else:\n",
        "          img, label = self.load_from_dataset(index)\n",
        "        img = self.trans_img(img) \n",
        "        return img, label \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe.index)\n",
        "    \n",
        "    def trans_img(self, img):\n",
        "        if self.training == True:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "                transforms.Resize((self.img_size, self.img_size)),\n",
        "                transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "                ])\n",
        "        else:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((self.img_size,self.img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "                ])\n",
        "        \n",
        "        return  transform(img)\n",
        "\n",
        "    def load_from_dataset(self, index):\n",
        "      filename = self.dataframe.iloc[index, 0]\n",
        "      label = self.dataframe.iloc[index, 1]\n",
        "      img = Image.open(self.rootPath + filename)\n",
        "      img = img.convert('RGB')\n",
        "      return img, label\n",
        "\n",
        "    def copy_paste(self, index):\n",
        "      classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
        "      target = random.sample(classes, 1)[0]\n",
        "      if target == 'NoF':\n",
        "        # mask the fish part in an image of other category\n",
        "        classes2 = ['ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
        "        source = random.sample(classes2, 1)[0]\n",
        "        source_dir = os.path.join('/content/train', source)\n",
        "        files = [x for x in sorted(os.listdir(source_dir))]\n",
        "        filename = random.sample(files, 1)[0]\n",
        "        img = Image.open(os.path.join(source_dir, filename))\n",
        "        labelname = filename[:-4] + '.json'\n",
        "        if not os.path.isfile(os.path.join('/content/label/' + source, labelname)):\n",
        "          return self.load_from_dataset(index)\n",
        "        with open(os.path.join('/content/label/' + source, labelname)) as json_file:\n",
        "          labels = json.load(json_file)\n",
        "        for l in labels[\"shapes\"]:\n",
        "          xmin = int(min((l[\"points\"][0][0], l[\"points\"][1][0])))\n",
        "          xmax = int(max((l[\"points\"][0][0], l[\"points\"][1][0])))\n",
        "          ymin = int(min((l[\"points\"][0][1], l[\"points\"][1][1])))\n",
        "          ymax = int(max((l[\"points\"][0][1], l[\"points\"][1][1])))\n",
        "          mask = Image.new(\"RGB\", (xmax-xmin, ymax-ymin), (0, 0, 0))\n",
        "          img.paste(mask, (xmin, ymin))\n",
        "      else:\n",
        "        # paste fish patches of one category on a NoF image\n",
        "        source_dir = os.path.join('/content/train', target)\n",
        "        files = [x for x in sorted(os.listdir(source_dir))]\n",
        "        filename = random.sample(files, 1)[0]\n",
        "        fish_img = Image.open(os.path.join(source_dir, filename))\n",
        "        labelname = filename[:-4] + '.json'\n",
        "        if not os.path.isfile(os.path.join('/content/label/' + target, labelname)):\n",
        "          return self.load_from_dataset(index)\n",
        "        with open(os.path.join('/content/label/' + target, labelname)) as json_file:\n",
        "          labels = json.load(json_file)\n",
        "\n",
        "        nof_dir = os.path.join('/content/train', 'NoF')\n",
        "        background_files = [x for x in sorted(os.listdir(nof_dir))]\n",
        "        background_filename = random.sample(background_files, 1)[0]\n",
        "        img = Image.open(os.path.join(nof_dir, background_filename))\n",
        "        for l in labels[\"shapes\"]:\n",
        "          xmin = int(min((l[\"points\"][0][0], l[\"points\"][1][0])))\n",
        "          xmax = int(max((l[\"points\"][0][0], l[\"points\"][1][0])))\n",
        "          ymin = int(min((l[\"points\"][0][1], l[\"points\"][1][1])))\n",
        "          ymax = int(max((l[\"points\"][0][1], l[\"points\"][1][1])))\n",
        "          fish_patch = fish_img.crop((xmin, ymin, xmax, ymax))\n",
        "          width, height = img.size\n",
        "          x = random.randint(0, width - (xmax - xmin) + 1)\n",
        "          y = random.randint(0, height - (ymax - ymin) + 1) \n",
        "          img.paste(fish_patch, (x, y))\n",
        "\n",
        "      return img, classes.index(target)\n"
      ],
      "metadata": {
        "id": "JsvHMKXdrhCj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset"
      ],
      "metadata": {
        "id": "BeFoIJfjtY2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 43\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=seed,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "root = '/content/train'\n",
        "train_data = fish_dataset(df, rootPath=root, training=True)\n",
        "train_dataloader = DataLoader(train_data, batch_size=hyper[\"batch_size\"], shuffle=True)"
      ],
      "metadata": {
        "id": "E04mv2pttbIK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVpouLat1tvx",
        "outputId": "0676ae89-8610-4841-8a0d-02b85dfb19a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "kB1jzUwExaTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hgZAQwHxZTE",
        "outputId": "6f6832c4-ed39-4460-cb9d-5caae9b3a0ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = models.resnet101(pretrained=True)\n",
        "model = models.regnet_y_8gf(pretrained=True)\n",
        "num_ftrs = model.fc.in_features # number of neuron that input to last FC\n",
        "model.fc = nn.Linear(num_ftrs, hyper[\"class_num\"])\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "8JSZyOHDxZtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "tAWM5ycMx_Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=hyper[\"learning_rate\"], momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=hyper[\"step_size\"], gamma=0.8, verbose=True)"
      ],
      "metadata": {
        "id": "6B1EspQcyDKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7916a28d-e9b6-414f-9661-69248ee96bd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Rii4gkOdyrhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(train_loss_history, epoch):\n",
        "  epoch_history = [*range(0, epoch+1, 1)]\n",
        "  line1, = plt.plot(epoch_history, train_loss_history ,label = 'Training')\n",
        "  plt.legend(handles = [line1])\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.savefig('loss.png')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "55V3OqkH0nSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = 'log.txt'\n",
        "train_loss_history=[]\n",
        "\n",
        "for epoch in range(hyper[\"epoch\"]):\n",
        "\n",
        "  \"\"\"=====train one epoch=====\"\"\"\n",
        "  model.train()\n",
        "  totalLoss = 0\n",
        "  count = 0\n",
        "  correct_count = 0\n",
        "  for x, label in train_dataloader:\n",
        "      x = x.to(device)\n",
        "      label = label.to(device).type(torch.long)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(x)\n",
        "      loss = criterion(output, label)\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      count += len(x)\n",
        "      correct_count += (predicted == label).sum().item()\n",
        "      totalLoss += loss.item()*len(label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  train_loss = totalLoss / count\n",
        "  accuracy = correct_count / count\n",
        "  train_loss_history.append(train_loss)\n",
        "\n",
        "  with open(log_path, 'a') as f:\n",
        "      f.write(\"Epoch {}: Training Loss: {:.4f}, accuracy: {:.4f}%\\n\".format(epoch+1, train_loss, 100*accuracy))\n",
        "  print(\"Epoch {}: Training Loss: {:.4f}, accuracy: {:.4f}%\".format(epoch+1, train_loss, 100*accuracy))\n",
        "\n",
        "  torch.save(model.state_dict(), \"model_ep{}_loss{:.4f}.pkl\".format(epoch+1, train_loss))\n",
        "\n",
        "  print(\"-------\")\n",
        "\n",
        "  plot_loss(train_loss_history, epoch)\n",
        "  lr_scheduler.step()"
      ],
      "metadata": {
        "id": "RvKHleTlyq8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}